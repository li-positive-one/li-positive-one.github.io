---
layout: post
title: "增强学习之DQN"
comments: true
description: "增强学习之DQN"
keywords: "增强学习 DQN"
---

## DQN是什么

### 基础

#### 价值函数

记回报为
$$
G_t=R_{t+1}+\lambda R_{t+2}+...=\sum_{k=0}^{\infty}\lambda^kR_{t+k+1}
$$
则价值函数为：
$$
v(s)=E[G_t|S_t=s]
$$

#### Bellman方程

$$
v(s)=E[R_{t+1}+\lambda v(S_{t+1})|S_t=s]
$$

当前状态的价值和下一步的价值以及当前的反馈Reward有关。

#### 动作价值函数:Q-function

$$
Q^{\pi}(s,a)=E[r_{t+1}+\lambda r_{t+2}+{\lambda}^2 r_{k+3}+ ...|s,a]\\
=E_{s'}[r+\lambda Q^\pi(s',a')|s,a]
$$

#### 最优动作价值函数

$$
Q^*(s,a)=\max_{\pi}Q^{\pi}(s,a)\\
=E_{s'}[r+\lambda \max_{a'}Q^\pi(s',a')|s,a]
$$

### 算法

#### 策略迭代

Policy Iteration的目的是通过迭代计算value function 价值函数的方式来使policy收敛到最优。

$$
v_{k+1}(s)=E_{\pi}[R_{t+1}+\gamma  v_k(S_{t+1})|S_t=s]\\
=\sum_{a}\pi(a|s)\sum_{s',r}p(s',t|s,a)[r+\gamma v_k(s')]
$$

Policy Iteration一般分成两步：

1. Policy Evaluation 策略评估。目的是 更新Value Function，或者说更好的估计基于当前策略的价值
2. Policy Improvement 策略改进。 使用 greedy policy 产生新的样本用于第一步的策略评估

#### 价值迭代

从如下形式的式子

$$
v_*(s)=\max_a E[R_{t+1}+\gamma v_*(s_{t+1})|S_t=s,A_t=a]\\
=\max_a\sum_{s',r}p(s',r|s,a)[r+\gamma v_*(s')]
$$

可以得到

$$
v_{k+1}(s)=\max_a E[R_{t+1}+\gamma v_k(s_{t+1})|S_t=s,A_t=a]\\
=\max_a\sum_{s',r}p(s',r|s,a)[r+\gamma v_k(s')]
$$

这是基于价值函数的迭代。

以下是基于动作价值函数的迭代。

$$
Q_{i+1}(s,a)=E_{s'}[r+\lambda \max_{a'}Q_i(s',a')|s,a]
$$

###  参考：

[Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)

[DQN从入门到放弃](https://zhuanlan.zhihu.com/p/21262246?refer=intelligentunit)



