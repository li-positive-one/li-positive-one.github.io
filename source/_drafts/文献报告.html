---
title: Zotero 报告测试
categories:
  - 文献报告
tags:
  - Test
date: 2017-05-15
---

<!DOCTYPE html>
<html><head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>Zotero 报告</title>
		<link rel="stylesheet" type="text/css" href="1_files/detail.css">
		<link rel="stylesheet" type="text/css" media="screen,projection" href="1_files/detail_screen.css">
		<link rel="stylesheet" type="text/css" media="print" href="1_files/detail_print.css">
	</head>
	<body>
		<ul class="report combineChildItems">
			<li id="item_9YKTFMXN" class="item journalArticle">
			<h2>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation</h2>
				<table>
					<tbody><tr>
						<th>类型</th>
						<td>期刊文章</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Özgün Çiçek</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Ahmed Abdulkadir</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Soeren S. Lienkamp</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Thomas Brox</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Olaf Ronneberger</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1606.06650">http://arxiv.org/abs/1606.06650</a></td>
					</tr>
					<tr>
					<th>期刊</th>
						<td>arXiv:1606.06650 [cs]</td>
					</tr>
					<tr>
					<th>日期</th>
						<td>2016-06-21</td>
					</tr>
					<tr>
					<th>其它</th>
						<td>arXiv: 1606.06650</td>
					</tr>
					<tr>
					<th>访问时间</th>
						<td>2019/10/25 上午12:49:02</td>
					</tr>
					<tr>
					<th>馆藏目录</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>摘要</th>
						<td>This paper introduces a network for volumetric segmentation 
that learns from sparsely annotated volumetric images. We outline two 
attractive use cases of this method: (1) In a semi-automated setup, the 
user annotates some slices in the volume to be segmented. The network 
learns from these sparse annotations and provides a dense 3D 
segmentation. (2) In a fully-automated setup, we assume that a 
representative, sparsely annotated training set exists. Trained on this 
data set, the network densely segments new volumetric images. The 
proposed network extends the previous u-net architecture from 
Ronneberger et al. by replacing all 2D operations with their 3D 
counterparts. The implementation performs on-the-fly elastic 
deformations for efficient data augmentation during training. It is 
trained end-to-end from scratch, i.e., no pre-trained network is 
required. We test the performance of the proposed method on a complex, 
highly variable 3D structure, the Xenopus kidney, and achieve good 
results for both use cases.</td>
					</tr>
					<tr>
					<th>短标题</th>
						<td>3D U-Net</td>
					</tr>
					<tr>
					<th>添加日期</th>
						<td>2019/10/25 上午12:49:02</td>
					</tr>
					<tr>
					<th>修改日期</th>
						<td>2019/10/25 上午12:49:02</td>
					</tr>
				</tbody></table>
				<h3 class="tags">标签：</h3>
				<ul class="tags">
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
				</ul>
				<h3 class="notes">笔记：</h3>
				<ul class="notes">
					<li id="item_AVEZ6KIW">
<p class="plaintext">Comment: Conditionally accepted for MICCAI 2016</p>
					</li>
				</ul>
				<h3 class="attachments">附件</h3>
				<ul class="attachments">
					<li id="item_XN452JRE">arXiv.org Snapshot					</li>
					<li id="item_4RZRIRE9">Çiçek et al_2016_3D U-Net.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_4RZRIRE9/1">1 Introduction</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_4RZRIRE9/3">1.1 Related Work</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4RZRIRE9/3">2 Network Architecture</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_4RZRIRE9/4">3 Implementation Details</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_4RZRIRE9/4">3.1 Data</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4RZRIRE9/5">3.2 Training</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_4RZRIRE9/5">4 Experiments</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_4RZRIRE9/5">4.1 Semi-Automated Segmentation</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4RZRIRE9/7">4.2 Fully-automated Segmentation</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4RZRIRE9/7">5 Conclusion</a></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_HHPA5J38" class="item journalArticle">
			<h2>Neural Ordinary Differential Equations</h2>
				<table>
					<tbody><tr>
						<th>类型</th>
						<td>期刊文章</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Ricky T. Q. Chen</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Yulia Rubanova</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Jesse Bettencourt</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>David Duvenaud</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1806.07366">http://arxiv.org/abs/1806.07366</a></td>
					</tr>
					<tr>
					<th>期刊</th>
						<td>arXiv:1806.07366 [cs, stat]</td>
					</tr>
					<tr>
					<th>日期</th>
						<td>2019-01-14</td>
					</tr>
					<tr>
					<th>其它</th>
						<td>arXiv: 1806.07366</td>
					</tr>
					<tr>
					<th>访问时间</th>
						<td>2019/10/25 下午3:13:27</td>
					</tr>
					<tr>
					<th>馆藏目录</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>摘要</th>
						<td>We introduce a new family of deep neural network models. 
Instead of specifying a discrete sequence of hidden layers, we 
parameterize the derivative of the hidden state using a neural network. 
The output of the network is computed using a black-box differential 
equation solver. These continuous-depth models have constant memory 
cost, adapt their evaluation strategy to each input, and can explicitly 
trade numerical precision for speed. We demonstrate these properties in 
continuous-depth residual networks and continuous-time latent variable 
models. We also construct continuous normalizing flows, a generative 
model that can train by maximum likelihood, without partitioning or 
ordering the data dimensions. For training, we show how to scalably 
backpropagate through any ODE solver, without access to its internal 
operations. This allows end-to-end training of ODEs within larger 
models.</td>
					</tr>
					<tr>
					<th>添加日期</th>
						<td>2019/10/25 下午3:13:27</td>
					</tr>
					<tr>
					<th>修改日期</th>
						<td>2019/10/25 下午3:13:27</td>
					</tr>
				</tbody></table>
				<h3 class="tags">标签：</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
					<li>Statistics - Machine Learning</li>
				</ul>
				<h3 class="notes">笔记：</h3>
				<ul class="notes">
					<li id="item_9TQJVEDQ">
<div><p>用ODE来做图像分类或者实现其他任务</p>
<p>dz/dt=f(t,z,lambda)</p>
<p>f可以是一个以lambda为参数的神经网络。</p>
<p>然后z(t_1)就是网络的输出</p>
<p>可以用反传来求loss关于参数的导数</p>
<p>创新点1是前传过程中不用记录每步的输出，在反传时现算即可，从而实现O(1)的内存占用。</p>
<p>&nbsp;</p></div>
					</li>
				</ul>
				<h3 class="attachments">附件</h3>
				<ul class="attachments">
					<li id="item_VX8F3KIP">arXiv.org Snapshot					</li>
					<li id="item_N4C6635U">Chen et al_2019_Neural Ordinary Differential Equations.pdf					</li>
				</ul>
			</li>


			<li id="item_JVND6HEH" class="item journalArticle">
			<h2>PolyNet: A Pursuit of Structural Diversity in Very Deep Networks</h2>
				<table>
					<tbody><tr>
						<th>类型</th>
						<td>期刊文章</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Xingcheng Zhang</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Zhizhong Li</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Chen Change Loy</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Dahua Lin</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1611.05725">http://arxiv.org/abs/1611.05725</a></td>
					</tr>
					<tr>
					<th>期刊</th>
						<td>arXiv:1611.05725 [cs]</td>
					</tr>
					<tr>
					<th>日期</th>
						<td>2017-07-17</td>
					</tr>
					<tr>
					<th>其它</th>
						<td>arXiv: 1611.05725</td>
					</tr>
					<tr>
					<th>访问时间</th>
						<td>2019/10/25 下午3:13:30</td>
					</tr>
					<tr>
					<th>馆藏目录</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>摘要</th>
						<td>A number of studies have shown that increasing the depth or 
width of convolutional networks is a rewarding approach to improve the 
performance of image recognition. In our study, however, we observed 
difficulties along both directions. On one hand, the pursuit for very 
deep networks is met with a diminishing return and increased training 
difficulty; on the other hand, widening a network would result in a 
quadratic growth in both computational cost and memory demand. These 
difficulties motivate us to explore structural diversity in designing 
deep networks, a new dimension beyond just depth and width. 
Specifically, we present a new family of modules, namely the 
PolyInception, which can be flexibly inserted in isolation or in a 
composition as replacements of different parts of a network. Choosing 
PolyInception modules with the guidance of architectural efficiency can 
improve the expressive power while preserving comparable computational 
cost. The Very Deep PolyNet, designed following this direction, 
demonstrates substantial improvements over the state-of-the-art on the 
ILSVRC 2012 benchmark. Compared to Inception-ResNet-v2, it reduces the 
top-5 validation error on single crops from 4.9% to 4.25%, and that on 
multi-crops from 3.7% to 3.45%.</td>
					</tr>
					<tr>
					<th>短标题</th>
						<td>PolyNet</td>
					</tr>
					<tr>
					<th>添加日期</th>
						<td>2019/10/25 下午3:13:30</td>
					</tr>
					<tr>
					<th>修改日期</th>
						<td>2019/10/25 下午3:13:30</td>
					</tr>
				</tbody></table>
				<h3 class="tags">标签：</h3>
				<ul class="tags">
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
				</ul>
				<h3 class="notes">笔记：</h3>
				<ul class="notes">
					<li id="item_9YKI5H9Z">
<p class="plaintext">Comment: Tech report</p>
					</li>
				</ul>
				<h3 class="attachments">附件</h3>
				<ul class="attachments">
					<li id="item_G4I7AULY">arXiv.org Snapshot					</li>
					<li id="item_EJ2JUY9G">Zhang et al_2017_PolyNet.pdf					</li>
				</ul>
			</li>


			<li id="item_W649S7II" class="item journalArticle">
			<h2>Three-dimensional propagation and time-reversal of fluorescence images</h2>
				<table>
					<tbody><tr>
						<th>类型</th>
						<td>期刊文章</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Yichen Wu</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Yair Rivenson</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Hongda Wang</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Yilin Luo</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Eyal Ben-David</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Aydogan Ozcan</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1901.11252">http://arxiv.org/abs/1901.11252</a></td>
					</tr>
					<tr>
					<th>期刊</th>
						<td>arXiv:1901.11252 [physics]</td>
					</tr>
					<tr>
					<th>日期</th>
						<td>2019-01-31</td>
					</tr>
					<tr>
					<th>其它</th>
						<td>arXiv: 1901.11252</td>
					</tr>
					<tr>
					<th>访问时间</th>
						<td>2019/10/25 上午12:47:20</td>
					</tr>
					<tr>
					<th>馆藏目录</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>摘要</th>
						<td>Unlike holography, fluorescence microscopy lacks an image 
propagation and time-reversal framework, which necessitates scanning of 
fluorescent objects to obtain 3D images. We demonstrate that a neural 
network can inherently learn the physical laws governing fluorescence 
wave propagation and time-reversal to enable 3D imaging of fluorescent 
samples using a single 2D image, without mechanical scanning, additional
 hardware, or a trade-off of resolution or speed. Using this data-driven
 framework, we increased the depth-of-field of a microscope by 20-fold, 
imaged Caenorhabditis elegans neurons in 3D using a single fluorescence 
image, and digitally propagated fluorescence images onto user-defined 3D
 surfaces, also correcting various aberrations. Furthermore, this 
learning-based approach cross-connects different imaging modalities, 
permitting 3D propagation of a wide-field fluorescence image to match 
confocal microscopy images acquired at different sample planes.</td>
					</tr>
					<tr>
					<th>添加日期</th>
						<td>2019/10/25 上午12:47:20</td>
					</tr>
					<tr>
					<th>修改日期</th>
						<td>2019/10/25 上午12:47:20</td>
					</tr>
				</tbody></table>
				<h3 class="tags">标签：</h3>
				<ul class="tags">
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
					<li>Computer Science - Machine Learning</li>
					<li>Physics - Applied Physics</li>
					<li>Physics - Optics</li>
				</ul>
				<h3 class="notes">笔记：</h3>
				<ul class="notes">
					<li id="item_MX7L7U78">
<p class="plaintext">Comment: 39 pages (main text)</p>
					</li>
				</ul>
				<h3 class="attachments">附件</h3>
				<ul class="attachments">
					<li id="item_DD86LCY6">arXiv.org Snapshot					</li>
					<li id="item_NFWPP554">Wu et al_2019_Three-dimensional propagation and time-reversal of fluorescence images.pdf					</li>
				</ul>
			</li>

		</ul>
	
</body></html>