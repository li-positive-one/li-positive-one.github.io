---
layout: post
title: "一阶随机优化方法总结"
comments: true
description: "随机梯度下降法总结"
keywords: "随机梯度下降 深度学习"
---



此处的一阶方法是指只用到导数的方法，而非严格的只进行一阶近似的方法。

### 记号

函数：$f$

梯度：$g=\nabla f$

参数：$\theta$

用脚标$t$表示第$t$步。

#### SGD

普普通通，寻常无比的随机梯度下降法，最简单的方法。

$$
\theta_{t+1}=\theta_{t+1}-\alpha g(\theta_t)
$$

#### Momentum

$$
v_t=\gamma v_{t-1}+\alpha g(\theta_t)\\
\theta_{t+1}=\theta_{t+1}-v_t\\
$$

引入一个动量项，Michael Jordan在北大的演讲提到过其直观原理，迭代法对应的一个微分方程从1阶上升到了2阶。（可以想象一个小球在有阻尼情况下从碗边滚到碗中央，SGD对应的是匀速情景，Momentum对应的是有重力情景）数学原理请参考Nesterov加速。

####  Nesterov accelerated gradient

$$
v_t=\gamma v_{t-1}+\alpha g(\theta_t-\gamma v_{t-1})\\
\theta_{t+1}=\theta_{t+1}-v_t\\
$$

这个方法与Momentum方法的区别在于，NAG用来更新动量的梯度，选取的是假设按照上一时刻动量走了一小段的位置的梯度。个人理解是大概类似于微分方程的不同离散格式。

#### Adagrad

#### Adadelta

大概算是一种拟牛顿法？

#### Adam

$$
m_t=\mu*m_{t-1}+(1-\mu)*g(\theta_t)\\
n_t=v*n_{t-1}+(1-\upsilon)*g(\theta_t)’*g(\theta_t)\\
\hat m_t =\frac{m_t}{1-\mu^t}\\
\hat n_t =\frac{n_t}{1-\upsilon^t}\\
\theta_{t+1}=\theta_{t}-\eta*\frac{\hat m_t}{\sqrt{\hat n_t}+\epsilon}
$$





















