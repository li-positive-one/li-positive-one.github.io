---
layout: post
title: "大数据分析中的算法-作业5"
comments: true
description: "大数据分析中的算法-作业5"
keywords: "大数据分析中的算法 随机梯度下降"
---

### 实现随机优化方法

From：[ homework5](http://bicmr.pku.edu.cn/~wenzw/bigdata/homework-sto.pdf)

#### 不同种类的优化算法:

Adadelta, AdagradDA, Adagrad, ProximalAdagrad,
Ftrl, Momentum, adam, Momentum, CenteredRMSProp, nesterov, rmsprop,SAG, SAGA, SVRG

#####  我所了解的：

Adadelta， Adagrad，Momentum, Adam, nesterov

##### 尚未了解的：

Ftrl,AdagradDA，ProximalAdagrad, SAG, SAGA, SVRG，CenteredRMSProp, rmsprop,



####  实验数据集：

##### MNIST
http://yann.lecun.com/exdb/mnist/
https://github.com/mrgloom/MNIST-dataset-in-different-formats/tree/master/data
##### Covertype

https://archive.ics.uci.edu/ml/datasets/Covertype

https://github.com/DartML/Stein-Variational-Gradient-Descent/blob/master/data/covertype.mat



#### 我实现的：

Adam，SGD，SGD-BB，nesterov-Momentum，nesterov-MomentumBB

其中BB算法对于SGD和nesterov-Momentum有明显的加速。对Adam同样实验过BB算法，但是效果十分不好，结果出现反复震荡，我的理解是Adam本身就在不同的维度上以不同速率更新，如果外加一个BB步长可能会受到破坏，也有可能是我加BB步长的方法不对。



### 探讨点

#### BB算法的加速效果

对于SGD和nesterov-Momentum有明显的加速。

#### l1惩罚项的效果

只有当数据量较小时（此时容易出现过拟合）l1惩罚项才会有缩小训练集acc与测试集acc的效果。

当数据量大时，根本不会出现过拟合现象，加惩罚项反而会导致训练集和测试集的acc都下降。

（对于mnist来说，当N=500时有明显效果）

#### 三种算法的对比

Adam对于初始参数要求不高。SGD和nesterov-Momentum十分要求参数，但是使用BB步长后反而对参数不敏感。

#### 数据量对算法效果的影响

当数据量较大时，再提升数据量不能提高acc，猜测是此时acc已经受限于模型本身。

#### 不同数据集

MNIST更容易取得更高的accuracy

Covertype虽然维度低，当相应的模型中的参数也少，所以难以取得更好的效果。



