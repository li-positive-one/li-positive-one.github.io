---
layout: post
title: "压缩感知和稀疏优化基本理论"
comments: true
description: "压缩感知和稀疏优化基本理论"
keywords: "压缩感知和稀疏优化基本理论"
---

# 压缩感知和稀疏优化基本理论



#### 欠定线性系统

- $x \in R^n,A \in R^{m\times n},b \in R^m$

当方程数量比未知数少时

- 代数基本定理告诉我们，我们无法找到解ｘ
- 一般情况下，这是绝对正确的

#### 特殊结构

［此处应有图］

如果未知数被假定为

- 稀疏的
- 低秩的

那么我们常常可以对这类问题使用凸优化找到解

#### 压缩感知

[此处应有图]

#### 线性规划公式

l0 最优


$$
\min ||x||_{l_0}\\
s.t. Ax=b\\
Combinatoriall~ hard
$$
l1最优
$$
\min ||x||{l_1}\\
s.t. Ax=b\\
线性规划
$$






#### 压缩感知

- 由David Donoho命名
- 成为稀疏信号重建的标签
- But really one instance of underdetermined problems
- Informs analysis of underdetermined problems
- Changes viewpoint about underdetermined problems
- Starting point of a general burst of activity in
  - information theory
  - signal processing
  - statistics
  - some areas of computer science
  - ．．．
- Inspired new areas of research, e. g. low-rank matrix recovery



#### 一个现代悖论

- Massive data acquisition
- Most of the data is redundant and can
  be thrown away
- Seems enormously wasteful

#### Sparsity in signal processing

［图］

#### 稀疏和小波＂压缩＂

Take a mega-pixel image

- Compute 1, 000, 000 wavelet coefficients
- Set to zero all but the 25, 000 largest coefficients
- Invert the wavelet transform

This principle underlies modern lossy coders

#### Comparison

[图表]

####  How many measurements to acquire a sparse signal?

- x is s-sparse
- Take m random and nonadaptive
  measurements, e.g. ak ∼ N (0, I):
  yk =< ak, x >, k = 1, . . . , m
- Reconstruct by `1 minimization
- First fundamental result
  if m ≥ slog n
  Recovers original exactly
  Efficient acquisition is possible by nonadaptive sensing

Cannot do essentially better with

- fewer measurements
- with other reconstruction algorithms

[一页图]

#### Signals/images may not be exactly sparse

[一页图]

#### Nonadaptive sensing of compressible signals

#### But data are always noisy...



#### Opportunities

当测量是

- 昂贵的 (例如燃料电池成像，近红外成像)
- 慢的 (例如核磁共振)
- 超过当前能力 (宽带模数转换)
- 浪费的
- 丢失的

压缩感知可能提供一个好的解决方法

#### Fundamental Question

稀疏优化的基本问题是:
**我是否可以相信我的模型会返回一个预期的稀疏量吗?**
也就是

- 我的模型是否有唯一解? (否则，不同的算法可能会返回不同的解)


- 这个解是否严格等于原来的稀疏解?
- 如果不是(由于噪音)，那么是否是原来的稀疏量的一个可信的逼近?
- 数值求解这个模型有多困难？

#### How to read guarantees

Some basic aspects that distinguish different types of guarantees:

- 可恢复性 (精确的) vs 稳定性 (不精确的)
- 一般的 A 还是 特殊的 A?
- 普遍的 (所有稀疏向量) 或者有要求的 (特定的稀疏向量)?


- General optimality? or specific to model/algorithm?
- Required property of A: spark, RIP, coherence, NSP, dual

certificate?

- If randomness is involved, what is its role?
- Condition/bound is tight or not? Absolute or in order of

magnitude?

#### Spark

第一个问题是寻找 Ax = b的稀疏解．
稀疏解是唯一的吗? 在什么样的条件下?
任给一个稀疏的$ x$, 如果判断他是否是最稀疏的那一个?



------

定义(Donoho and Elad 2003)：
矩阵Ａ的spark是指Ａ的列向量中线性相关向量最小的个数，记做spark(A).

------

rank(A)是矩阵Ａ的列向量中线性无关向量最大的个数．一般来说,除了很多随机生成的矩阵, $spark(A) \neq rank(A) + 1$

矩阵的rank很容易计算，但是spark需要组合搜索得到．



------

定理(Gorodnitsky and Rao 1997)：
如果方程$Ax=b$有一个解$x$满足$||x||_0 < spark(A)/2$，那么$x$就是最稀疏的解．

------

证明：

如果存在一个解$y$ 也满足 $ Ax=b$ 且$x-y \neq 0$，那么$A(x-y)=0$且有
$$
||x||_0+||y||_0 \geqslant ||x-y||_0 \geqslant spark(A),
$$
即$||y||_0 \geqslant spark(A)-||x||_0 > spark(A)/2>||x||_0$

- 这个结果不意味着我们可以高效的数值计算出结果
- 对于很多随机矩阵$A \in R^{m \times n}$，这个结果意味着如果一个算法得到的$x$满足$||x||_0 <(m+1)/2$ ，则$x$以概率１是最优点．
- 当$spark(A)$很难得到时该怎么办？

#### General Recovery - Spark

- 矩阵的秩很容易计算，但是spark需要组合搜索得到．

- 但是，对于具有一般元素的矩阵，$spark(A) ＝ rank(A) + 1$

- 例如，如果矩阵$A \in R^{m \times n}(m<n)$有元素$A_{ij} \sim \mathcal{N}(0,1)$,则$ rank(A) ＝m=spark(A)－ 1$以概率１成立

- 通常的，任何一个满秩矩阵$A \in R^{m \times n}(m<n)$，任何 $m+1$列是线性无关的，所以 

- $$
  spark(A)\leq m+1 =rank(A)+1
  $$




#### Coherence

------

定义(Mallat and Zhang 1993)：

一个矩阵A 的coherence是A的两列内积的绝对值最大值。假设$A=[a_1,a_2,...,a_n]$.

矩阵A的mutual coherence由下式给出：
$$
\mu(A)=\max_{k,j,k\neq j} \frac{|a_k^Ta_j|}{||a_k||_2 \cdot ||a_j||_2}
$$

------

- coherence度量了A的列之间的相关性
- 对于酉矩阵，$\mu(A)=0$
- 对于重建问题，我们希望一个小的$\mu(A)=0$，即A接近于一个酉矩阵。
- 对于$A=[\Psi,\Phi]$,$\Psi$,$\Phi$是$n\times n$ 酉矩阵，那么有$n^{-1/2}\leq \mu(A) \leq 1$.注意当$A=[I,Foutier],[I,Hadamard]$时等式可以成立。
- 如果$A\in \mathbb{R}^{m \times n},n > m$,那么 $\mu(A) \geq m^{-1/2}$.

------

定理 (Donoho and Elad 2003)
$$
spark(A) \geq 1+ \mu^{-1}(A)
$$

------

证明提纲：

- $\bar{A} \leftarrow A $,把列向量在2范数下归一化
- $p \leftarrow spark(A)$
- $B \leftarrow a$ $ p \times p$ minor of $\bar{A}^T \bar{A}$
- $|B_{ii}|=1$以及$\sum_{i \neq j}|B_{ij}|\leq(p-1)\mu(A)$
- 假设$p<1+\mu^{-1}(A)$，即$1 > (p-1)\mu(A)$,则$|B_{ii}|>\sum_{j \neq i}|B_{ij}|,\forall i$
- 由$B \succ0 $((Gershgorin circle theorem)，而$spark(A)>p$.矛盾


#### Coherence-base guarantee

------

推论

------

与之前的定理对比



#### Coherence-based l0 = l1

定理 (Donoho and Elad 2003, Gribonval and Nielsen 2003)

如果A的列都已经规范化过了，且$Ax=b$有解$x$满足$||x||_0\leq(1+\mu^{-1})/2$，则x在l0和l1意义下都是最优解。

Proof Sketch：

- 我们已知x是唯一的l0最小点，令$S:=supp(x)$
- 假设$y$是l1最优解但不等于x，则考虑$h:=y-x$
- $h$一定满足$Ah=0$且



#### The null space of A



#### Restricted isometries: C. and Tao (04)



#### Interlude: when does sparse recovery make sense?



#### Equivalent view of restricted isometry property



#### With a picture



#### Formal equivalence



#### More Comments



#### Characterization of `1 solutions



#### Geometric picture



#### Characterization via KKT conditions



#### Subgradient



#### Optimality conditions II



#### Uniqueness



#### Sufficient conditions



#### Why this special dual certificate



#### General setup



#### General signal recovery





#### General signal recovery from noisy data



#### Proof of noisy recovery result



#### Preliminaries: Lemma 1



#### Preliminaries: Lemma 2



#### Preliminaries: Lemma 3



#### Preliminaries: Lemma 4

Proof:

Note that $h=h_T+h_{T^c}​$, then $||h||_2 \leqslant ||h_T||_2+||h_{T^c}||_2​$ . Let $T_j​$ be defined similarly as Lemma 2, then we have
$$
||H_{T^c}||_2=||\sum_{j \geqslant2}h_{T_j}||_2 \leqslant \sum_{j \geqslant2}||h_{T_j}||_2 \leqslant \frac{||h_{T^c_0}||_2}{\sqrt s}
$$
Since $||\hat x||_1 \leqslant ||x||_1$ , we obtain
$$
||x||_1 \geqslant ||x_{T_0}+h_{T_0}||_1+||x_{T_0^c}+h_{T_0^c}||_1
\geqslant ||x_{T_0}||_1-||h_{T_0}||_1+||h_{T_0^c}||_1-||x_{T_0^c}||_1
$$
Rearranging and again applying the triangle inequality
$$
||h_{T_0^c}||_1 \leqslant ||x||_1- ||x_{T_0}||_1+||h_{T_0}||_1+||x_{T_0^c}||_1
\leqslant  ||x -x_{T_0}||_1+||h_{T_0}||_1+||x_{T_0^c}||_1
$$

Hence ,we have $||h_{T_0^c}||_1\leqslant ||h_{T_0}||_1+2||x-x_s||_1$ , Therefore,
$$
||h_{T^c}||_2 \leqslant \frac{||h_{T_0}||_1+2||x-x_s||_1}{\sqrt s} \leqslant ||h_{T_0}||_2
+\frac{2||x-x_s||_1}{\sqrt s} 
$$
Lemma 3 gives
$$
||h_T||2 \leqslant \alpha \frac{||h_{T_0^c}||_1}{\sqrt s}+\beta \frac{| \langle Ah^T,Ah\rangle|}{||h_T||_2}\\
\leqslant   \alpha \frac{||h_{T_0}||_1+2||x-x_s||_1}{\sqrt s}+\beta \frac{| \langle Ah^T,Ah\rangle|}{||h_T||_2}\\
\leqslant \alpha ||h_{T_0}||_2+2\alpha \frac{|||x-x_s||_1}{\sqrt s}+\beta \frac{| \langle Ah^T,Ah\rangle|}{||h_T||_2}
$$
Using $||h_{t_0}||_2 \leqslant ||h_T||_2​$ gives
$$
(1-\alpha)||h_T||_2 \leqslant 2\alpha \frac{|||x-x_s||_1}{\sqrt s}+\beta \frac{| \langle Ah^T,Ah\rangle|}{||h_T||_2}
$$
Dividing by $1-\alpha$ gives
$$
||h||_2 \leqslant (\frac{4\alpha}{1-\alpha}+2)\frac{|||x-x_s||_1}{\sqrt s}+\frac{2\beta}{1-\alpha} \frac{| \langle Ah^T,Ah\rangle|}{||h_T||_2}
$$


