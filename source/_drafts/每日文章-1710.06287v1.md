---
title: "每日文章-A DEEP LEARNING APPROACH FOR RECONSTRUCTION FILTER KERNEL
DISCRETIZATION"
categories:
  - 读书笔记
tags:
  - Neutral network
  - Inverse problem
date: 2017-10-19
---

这篇论文的标题[One Network to Solve Them All — Solving Linear Inverse Problems](https://arxiv.org/abs/1703.09912)简单易懂：用一个网络解决所有线性反问题。

首先，带约束的线性反问题常见的形式都是这样的。

$$\min_x \frac{1}{2} \vert \vert y-Ax \vert \vert_2^2+\lambda\phi(x)$$

这样的问题显然可以使用ADMM算法求解。

$$\min_x \frac{1}{2} \vert \vert y-Az \vert \vert_2^2+\lambda\phi(x)\\x=z$$

在ADMM当中对x和z的更新实际上都是一个prox算子，但是对于z的更新是一个线性函数的prox，所以相当于与求解一个线性反问题，可以用共轭梯度法求解。而对于x的更新由于$\phi$的不同则有不同的形式。

这篇论文主要就是学习对$\phi$的更新的proximal算子。比如说这篇论文的实验主要集中在自然图像的去噪、

超分辨、压缩感知上，那么他就假设所有自然图像的Indicator function 是 $$\mathcal{I}_{\mathcal{x}}(\cdot)$$。然后他就要学习出来 $${prox}_{\mathcal{I}}$$ 。但是因为并不知道真实的 $$\mathcal{I}_{\mathcal{x}}(\cdot)$$ 是什么，所以他是用一个神经网络D进行训练，逼近这个函数。基于学习到的神经网络 $$D_l$$ ,他学习到了P也就是$${prox}_{\mathcal{I}}$$。

在训练过程中，使用了类似GAN的技术，及同时训练。P训练来蒙蔽D，D也要在训练以加强约束。

对于P的训练，它采用了这样的损失函数：

$$\min_{\theta_p} \sum_{x \in M,v \sim f(x)} \lambda_1\vert \vert x-P(x) \vert \vert ^2 +\lambda_2\vert \vert x-P(v)\vert \vert ^2+\lambda_3\vert \vert v-P(v)\vert \vert ^2-\lambda_4\log(\sigma(D_l \circ \mathcal{E}(v)))-\lambda_5\log(\sigma(D \circ P(x)))$$

其中$D$和$D_l$是分别工作在output (image) space and the latent spaces of the projector上的分类器。

参数$\lambda$ 分别是0.01, 1.0, 0.001, 0.0001, 0.001。

这个loss的前三项是prox算子中控制距离的项，后两项是函数值的项。前三项之所以不只留一项是因为希望对于优化过程中不同步的x（即刚开始优化时变量接近有nosing图片v，最后接近自然图片x）都有$P(x)-x$不要太大的约束。

具体的网络结构呢，D和$D_l$都是Resnet的变种，P则是autoencoder网络的变种。



等到网络训练好了，对于不同的反问题，例如去噪，超分辨等，就根据线性算子构造不同的ADMM迭代方法，带入我们习得的prox算子替代原始admm中的prox算子，就能取得One Network to Solve Them All的效果。

这个算法的优势主要就在于利用对抗学习的方法，学习到了一个不明函数$I(x)​$的prox算子$prox_{I}​$，这是传统方法所不具备的。比如对于自然图像问题，我们原来常用TV约束，但是tv约束的图像与自然图像范围其实有不小的差别，一些有纹理的自然图像tv很大，一些tv很小的图像却是cartoon化的，等等。所以感觉这个方法应用还是很大的。























