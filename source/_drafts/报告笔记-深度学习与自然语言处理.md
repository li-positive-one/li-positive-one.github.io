---
title: "深度学习与自然语言处理"
categories:
  - 报告笔记
tags:
  - NLP
  - Deep Learning
date: 2019-06-04
---

主题：深度学习与自然语言处理
报告人：百度 连义江
地点：北大二教201（深度学习课上报告）

## 深度学习概览

泛函极小化的角度看深度学习。
 
- 函数空间->参数化-> $$R^n$$ 中极小化问题。
- 无约束的极小值问题：非凸，初值，随机梯度下降。
- 机器学习不是一个纯粹的数学最优化问题。

机器学习之前的函数空间：线性函数，三角函数...
机器学习对应的函数空间：多层神经网络的模型。

## NLP问题

核心问题：如何来表达自然语言模型

什么是语言模型：
如何来估计一句话出现的概率：$$P(w_1,w_2,...w_m)$$
概率展开，截断

以前的建模方式：统计
- Bigram， trigram
- 没有泛化能力

DNN如何建模语言模型：
- word embedding
- cnn/rnn/lstm/gru
- transformer

## Word enbedding
如何来表达一个词的语义：
- 基于字面？
- 基于上下文分布？

DNN之前：
- 向量化表示词义：
  - N维欧式空间中向量
  -相似的词在空间中距离相近
- 把词表示为稀疏高位向量
  - 词表示为高维向量（上下文1，上下位2，。。。）
  - 上下文：
    - 词出现的文档
    - ngram
    - 。。。
- 把词表示为低维的稠密矩阵

基于DNN：
- 假设词有一个低维的向量表达
- 基于语言模型来建模
- 对模型做最大似然估计，word embedding 是一个副产品

基于循环神经网络的语言模型

之前的模型基于语言模型建模，词向量是副产品。下面的直接为了生成词向量来建模：
- CBOW直接基于上下文来预测当前词
- skip-gram基于当前词来预测上下文的词

矩阵分解方法和DNN方法的联系

### word embedding 的问题和意义
问题：
- 一词多义
- phrase embedding
- 窗口小的时候，good和bad语义相似了
- 可能所有的数字，所有的地名都很相似
意义：
- NLP网络结构不可或缺的一部分
  - 一般最底层结构都是word embedding
  - 可以是预训练好的，或者是联合训练的
- 在很多NLP任务中都产生了好的效果
- 成为NLP的一个相对独立的研究方向：
  - word2vec ，fasttext，elmo

word embedding: embedding 什么意思？

拓扑上的embedding：
- 空间的嵌入

## NLP的一般任务如何用DNN建模
常见NLP任务：
- 词性标注
- 专名识别：人名，地名
- 句法标注

归结于一个问题：
- 基于一个窗口上下文来预测每个词的label

传统做法：
- 为每个任务单独定义大量的特征，单独做人工标注数据
- 然后单独训练简单分类器模型

DNN做法：
- 几乎没有人工特征
- 直接基于词的低维表达来组合出context的低维表达
- 复杂模型，大量数据，通用模型

## NLP的另一个难点：表示学习
如何计算一个文档的语义表达？

一段话的语义由各组成部分的语义和他们之间的组合方法所确定。

DNN之前： 用词来表达，bag of words
基于DNN来做组合：CNN/RNN等

## Attention相关
条件语言概率模型

$$
P(Y|X)=\Pi_{i=1}^{N}P(Y_i|Y_{<i,X})
$$

两种流行的模型：
- encoder & decoder
  - encoder生成source的总结summary
  - decoder根据summary和已经生成的词翻译下一个词，逐词翻译
- attention

### self-attention，transformer

- CNN:
  - 依赖于局部窗口的有限个词
  - 容易并行
- RNN：
  - 通过hidden state间接建模依赖
  - 单向依赖建模，难以并行
- transformer：
  - 直接对依赖建模
  - 双向依赖建模，容易并行

## 深度学习和NLP趋势

- 训练方法：无监督+Fine-Tuning
  - 大规模数据集上无监督学习（10亿级别样本）
  - 小规模标记数据集上fine-tuning
  - 一个好的初值非常重要

- 模型结构的表达
  - 更大的模型容量
  - masked language model
  - 双向依赖模型 transformer

- 一个模型处理多个任务

### 一个典型的NLP任务
百度 搜索关键词广告 拍卖

- 如何来判断两个短文本之间的语义相似度
  - Query 鲜花快递
  - Bidword 鲜花速递

- 以前的做法：
  - 人工设计特征：miss/match等

- 现在的做法：
  - 完全抛弃人工特征
  - Transformer
  - 大规模无标记数据上做无监督的预训练
  - 小规模标记数据集上做监督训练 fine-tuning
  - 基于bert auc +3%


## 应用场景
@百度蜂巢

### 基于机器翻译的生成式触发

广告空间和query空间建立连接关系：
宝贝身上起红斑怎么办<->婴儿红疹治疗

传统做法：bool检索+改写
- 宝宝身上起红斑怎么办？ 
- 宝宝身上长红斑咋办？

新做法：生成式
精油什么牌子好-->精油十大品牌

机器翻译
一般的做法：phrase based translation
NMT： Neural Machine Translation 端到端

### 基于doc-embedding做向量式触发